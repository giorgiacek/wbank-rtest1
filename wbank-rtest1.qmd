---
title: "R Skills Assessment"
date: 2023-11-12
author: "Giorgia Cecchinato"
format:
  html:
    toc: true
    toc-location: right
    code-fold: true
---

```{r setup, include=FALSE}
library(fastverse)
library(readr)
library(rmarkdown)
library(DT)
library(stringr)
library(ggplot2)
library(purrr)
library(joyn)
library(dplyr)
```

## Data
Data for both assignments are loaded here.
```{r data}
tag      <- "202311081903"
base_url <- "https://github.com/randrescastaneda/pub_data/raw/"
data_url <- paste0(base_url, tag, "/data/Rtest1/")

wdi <-
  readr::read_rds(paste0(data_url, "wdi_in1.Rds"))

l_svy <- 
  readr::read_rds(paste0(data_url, "svy_sim_in1.Rds"))
```

## Basic Stats

### 1. Summary statistics of GDP per capita by region

This table shows some stats of GDP per capita by region and year. Total number of observations (not-weighted), mean, sd, min, and max.

```{r basic summary-stats}
wdi |> 
  fgroup_by(region, date) |>
  fsummarise(
    N = fnobs(gdp),
    mean = fmean(gdp, w = pop, na.rm=TRUE),
    SD = fsd(gdp, w = pop, na.rm=TRUE),
    Min = fmin(gdp, na.rm=TRUE),
    Max = fmax(gdp, na.rm=TRUE)
  ) |>
  datatable(
    filter = 'top', options = list(
  pageLength = 10, autoWidth = TRUE
)) |>
  formatRound(c('mean', 'SD', 'Min', 'Max'), 0)
```

### 2. Aggregate stats

In this table, I aggregate the `lifeex`, `gdp`, and `pov_intl` variables by `region` and `date`, using the mean, standard deviation, minimum, maximum, and median.

```{r basic aggregate-stats}
#| warning: false

wdi |>
  fselect(region, date, lifeex, gdp, pov_intl, pop) |>
  fgroup_by(region, date) |>
  fsummarise(across(c(lifeex, gdp, pov_intl), 
                    list(mean = fmean, sd = fsd, min = fmin, max = fmax), 
                    w = pop),
             pop = fsum(pop)) |>
  pivot(ids = c('region', 'date', 'pop'),
        values = c('lifeex_mean', 'lifeex_sd', 'lifeex_min', 'lifeex_max', 
                 'gdp_mean', 'gdp_sd', 'gdp_min', 'gdp_max', 
                 'pov_intl_mean', 'pov_intl_sd', 'pov_intl_min', 'pov_intl_max')) |>
  fmutate(variable2 = str_extract(variable, ".*?(?=_[^_]+$)"),
          estimate = str_extract(variable, "(mean|sd|min|max)$")) |>
  fselect(-variable) |>
  pivot(how = "wider",
        ids = c('region', 'date', 'estimate', 'pop'),
        names = c('variable2'),
        values = c('value')) |>
  fselect(estimate, region, date, pop, lifeex, gdp, pov_intl) |>
  DT::datatable(
    filter = 'top', options = list(
  pageLength = 10, autoWidth = TRUE
)) |>
  formatRound(c('lifeex', 'gdp', 'pov_intl'), 3)
```

### 3. Find Outliers
Here I find the outliers of `lifeex`, `gpd`, and `gini` by year above and below 2.5 standard deviations from the mean.

```{r basic outliers}
#| warning: false
#| message: false

# 1. Find yearly stats
yearly_stats <- wdi|>
  fgroup_by(date) |>
  fsummarise(
    mean_lifeex = fmean(lifeex, na.rm = TRUE, w = pop),
    sd_lifeex = fsd(lifeex, na.rm = TRUE, w = pop),
    mean_gdp = fmean(gdp, na.rm = TRUE, w = pop),
    sd_gdp = fsd(gdp, na.rm = TRUE, w = pop),
    mean_gini = fmean(gini, na.rm = TRUE, w = pop),
    sd_gini = fsd(gini, na.rm = TRUE, w = pop)
  )

# 2. Create outlier function
is_low_outlier <- function(value, mean, sd) {
  threshold = mean - 2.5 * sd
  return(value < threshold)
}

is_high_outlier <- function(value, mean, sd) {
  threshold = mean + 2.5 * sd
  return(value > threshold)
}

# 3. Find outliers
wdi_outliers <- wdi |>
  joyn::merge(yearly_stats, by = "date", verbose = FALSE) |> # had to try your package!
  fmutate(
    hl_lifeex = ifelse(!is.na(lifeex), is_high_outlier(lifeex, mean_lifeex, sd_lifeex), NA),
    ll_lifeex = ifelse(!is.na(lifeex), is_low_outlier(lifeex, mean_lifeex, sd_lifeex), NA),
    hl_gdp = ifelse(!is.na(gdp), is_high_outlier(gdp, mean_gdp, sd_gdp), NA),
    ll_gdp = ifelse(!is.na(gdp), is_low_outlier(gdp, mean_gdp, sd_gdp), NA),
    hl_gini = ifelse(!is.na(gini), is_high_outlier(gini, mean_gini, sd_gini), NA),
    ll_gini = ifelse(!is.na(gini), is_low_outlier(gini, mean_gini, sd_gini), NA)
  )

wdi_outliers |>
  fselect(date, region, lifeex, hl_lifeex, ll_lifeex, gdp, hl_gdp, ll_gdp, gini, hl_gini, ll_gini) |>
  DT::datatable(
    filter = 'top', options = list(
  pageLength = 5, autoWidth = TRUE
)) |>
  formatRound(c('lifeex', 'gdp', 'gini'), 3)

```

And this graph shows the outliers of `lifeex` by year.

```{r basic outliers graph}
wdi_outliers |>
  ggplot() +
  geom_point(aes(x = date, y = lifeex, color = region), size = 1) +
  geom_line(aes(x = date, y = mean_lifeex), linewidth = 0.5, colour = 'blue') +
  geom_ribbon(aes(x = date, ymin = mean_lifeex - sd_lifeex*2.5, 
                  ymax = mean_lifeex + sd_lifeex*2.5), alpha = 0.2)+
  theme_minimal()+
  labs(x = "Date", y = "Life Exp", color = NULL) +
  theme(legend.position=c(.5,.15), legend.direction = "horizontal")
```

## Simulated data

### 4. Poverty measures

Here I estimate the poverty headcount, poverty gap, and poverty severity--i.e., Foster-Greer-Thorbecke indices (FGT)--for each year using the global poverty lines of \$2.15, \$3.65, and \$6.85 in 2017 PPP prices.

The FGT indices are calculated as follows:

1.  **Poverty headcount**: the proportion of the population living below the poverty line.
2.  **Poverty gap**: the mean distance below the poverty line of the poor as a proportion of the poverty line.
3.  **Poverty severity**: the mean squared distance below the poverty line of the poor as a proportion of the poverty line.

```{r simulated FGT}
# Set poverty lines
poverty_lines <- c(2.15, 3.65, 6.85)

# Function to calculate FGT indices
calculate_FGT_indices_v2 <- function(survey, poverty_line) {
  survey %>%
    mutate(
      poor = ifelse(income <= poverty_line, 1, 0),
      poor_weight = poor * weight,
      poor_gap = ifelse(poor == 1, poverty_line - income, 0)
    ) %>%
    summarise(
      headcount = sum(poor_weight) / sum(weight),
      povgap = sum(poor_gap * poor_weight) / (poverty_line * sum(weight)),
      povseverity = sum((poor_gap / poverty_line)^2 * poor_weight) / sum(weight)
    )
}

# Apply function using map() and bind_rows()
svy_dt <- map(names(l_svy), ~{
  year <- .x
  survey_data <- l_svy[[year]]
  map(poverty_lines, ~{
    line <- .x
    fgt_results <- calculate_FGT_indices_v2(survey_data, line)
    data.table(year = year, pov_line = line, fgt_results)
  }) %>% bind_rows()
}) %>% bind_rows()

# Plot the table
svy_dt |>
  fmutate(year = str_remove(year, "Y")) |>
  datatable(
  filter = 'top', options = list(pageLength = 10, autoWidth = TRUE)) |>
  formatRound(c('headcount', 'povgap', 'povseverity'), 5)
```

And this graph shows the poverty headcount by year.
```{r simulated FGT graph}
svy_dt |>
  fmutate(year = str_remove(year, "Y")) |>
  ggplot(aes(x = year, y = headcount, 
             group = factor(pov_line),
             colour = factor(pov_line))) +
  geom_line(linewidth = 0.3) +
  geom_point(size = 1) +
  theme_minimal()+
  labs(x = "Year", y = "Headcount", color = NULL)+
  theme(legend.position='bottom')
```

### 5. Lorenz curve
Here I calculate the Lorenz curve at the percentile level.
```{r simulated Lorenz}

# Function ----
calculate_lorenz_data <- function(survey) {
  
  survey |>
    fmutate(cumsum_pop = fcumsum(weight),
            sum_pop = fsum(weight),
            cumsum_income = fcumsum(income * weight),
            sum_income = fsum(income * weight)) |>
    fmutate(cum_share_pop = cumsum_pop / sum_pop,
            cum_share_income = cumsum_income / sum_income) |>
    fmutate(percentile_group = floor(cum_share_pop * 100)) |>
    fgroup_by(percentile_group) |>
    fsummarise(
      cum_share_pop = first(cum_share_pop),
      cum_share_welfare = first(cum_share_income),
      welfare = first(income)) |>
    fmutate(bin = percentile_group) |>
    fsubset(bin > 0)
}

# Adjustments 
process_survey <- function(survey, year_name) {
  numeric_year <- as.numeric(sub("Y", "", year_name))
  lorenz_results <- calculate_lorenz_data(survey)
  
  # Add the year to the results
  lorenz_results$year <- numeric_year
  return(lorenz_results)
}

# Apply using map() and bind_rows()
lorenz_dt <- map(names(l_svy), ~process_survey(l_svy[[.x]], .x)) %>%
  bind_rows()

# Plot the table
lorenz_dt |>
  fselect(welfare, cum_share_welfare, cum_share_pop, year, bin) |>
  DT::datatable(
    filter = 'top', options = list(pageLength = 10, autoWidth = TRUE)) |>
    formatRound(c('welfare', 'cum_share_welfare', 'cum_share_pop'), 10)
```

And this graph shows the Lorenz curve by year.
```{r simulated lorenz graph}
lorenz_dt |>
  ggplot(aes(x = cum_share_pop, y = cum_share_welfare, 
             group = year, colour = factor(year))) +
  geom_line(linewidth = 0.3) +
  theme_minimal()+
  labs(x = "Cumulative Share of Population", y = "Cumulative Share of Welfare", color = NULL)+
  theme(legend.position=c(0.1, 0.6))
```

### 6. Gini coefficient
Here I calculate the Gini coefficient without using auxiliary functions.
```{r simulated gini}
# Function
calculate_gini_alternative <- function(survey) {
  # Sort the survey data by income
  survey <- survey[order(survey$income), ]
  
  # Calculate cumulative proportions
  survey$cum_pop <- cumsum(survey$weight) / sum(survey$weight)
  survey$cum_income <- cumsum(survey$income * survey$weight) / sum(survey$income * survey$weight)
  
  # Calculate Gini coefficient
  gini <- 1 - sum((survey$cum_income[-1] + survey$cum_income[-nrow(survey)]) * diff(survey$cum_pop))
  return(gini)
}


gini_results <- map(l_svy, calculate_gini_alternative)

# Convert the results to a data.table with year and Gini coefficient
gini_dt <- data.table(year = names(gini_results), gini_coefficient = unlist(gini_results))
gini_dt$year <- as.integer(sub("Y", "", gini_dt$year))

gini_dt |>
  DT::datatable(
    filter = 'top', options = list(pageLength = 10, autoWidth = TRUE)) |>
    formatRound(c('gini_coefficient'), 5)
```


```{r simulated gini graph}
gini_dt |>
  ggplot(aes(x = year, y = gini_coefficient)) +
  geom_line(linewidth = 0.3) +
  geom_point(size = 1) +
  theme_minimal()+
  labs(x = "Year", y = "Gini Coefficient")
```